{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "model_folder = Path(\"/Users/hudsonmendes/Models/pretrained\")\n",
    "data_folder = Path(\"/Users/hudsonmendes/Workspaces/hudsonmendes-estudos/cm3065-isp/exercise-04/files\")\n",
    "transcriptions_filepath = data_folder / \"Ex4_audio_files_transcriptions.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "\n",
    "def normalise_text(text):\n",
    "    text = \"\".join(c for c in unicodedata.normalize(\"NFD\", text) if unicodedata.category(c) != \"Mn\")\n",
    "    text = re.sub(\"[^A-Za-z0-9\\ ]+\", \"\", text)\n",
    "    text = re.sub(\"[\\s]+\", \" \", text)\n",
    "    return text.lower().strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(transcriptions_filepath, header=0)\n",
    "test_df[\"y_true\"] = test_df.y_true.map(normalise_text)\n",
    "test_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractclassmethod\n",
    "\n",
    "\n",
    "class AbstractASR:\n",
    "    @abstractclassmethod\n",
    "    def transcribe(self, filepath: Path) -> str:\n",
    "        raise NotImplementedError(\"You must implement the transcribe() method\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mozilla DeepSpeech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepspeech as ds\n",
    "import librosa as lr\n",
    "\n",
    "\n",
    "class DeepSpeechASR(AbstractASR):\n",
    "    model: ds.Model\n",
    "\n",
    "    def __init__(self, model_name: str, folder: Path, scorer_name: str = None):\n",
    "        super(DeepSpeechASR, self).__init__()\n",
    "        model_path = folder / f\"{model_name}.pbmm\"\n",
    "        self.model = ds.Model(str(model_path))\n",
    "        if not scorer_name:\n",
    "            scorer_name = model_name\n",
    "        scorer_path = folder / f\"{scorer_name}.scorer\"\n",
    "        if scorer_path.is_file():\n",
    "            self.model.enableExternalScorer(str(scorer_path))\n",
    "\n",
    "    def transcribe(self, filepath: Path):\n",
    "        audiofile = lr.load(filepath, sr=self.model.sampleRate())[0]\n",
    "        audiofile = (audiofile * 32767).astype(np.int16)\n",
    "        return self.model.stt(audiofile)\n",
    "\n",
    "\n",
    "asr_deepspeech_en = DeepSpeechASR(model_name=\"deepspeech-0.9.3-models\", folder=model_folder / \"deepspeech\")\n",
    "asr_deepspeech_it = DeepSpeechASR(model_name=\"output_graph_it\", folder=model_folder / \"deepspeech\")\n",
    "asr_deepspeech_es = DeepSpeechASR(\n",
    "    model_name=\"output_graph_es\", scorer_name=\"kenlm_es\", folder=model_folder / \"deepspeech\"\n",
    ")\n",
    "\n",
    "(\n",
    "    (\"en\", asr_deepspeech_en.transcribe(filepath=data_folder / test_df[test_df.locale == \"en\"].filename.values[0])),\n",
    "    (\"it\", asr_deepspeech_it.transcribe(filepath=data_folder / test_df[test_df.locale == \"it\"].filename.values[0])),\n",
    "    (\"es\", asr_deepspeech_es.transcribe(filepath=data_folder / test_df[test_df.locale == \"es\"].filename.values[0])),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpeechBrain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speechbrain.pretrained import EncoderDecoderASR\n",
    "\n",
    "\n",
    "class SpeechBrainASR:\n",
    "    def __init__(self, model_name: str, folder: Path):\n",
    "        super(SpeechBrainASR, self).__init__()\n",
    "        self.model = EncoderDecoderASR.from_hparams(source=f\"speechbrain/{model_name}\", savedir=folder)\n",
    "\n",
    "    def transcribe(self, filepath: Path) -> str:\n",
    "        return self.model.transcribe_file(str(filepath))\n",
    "\n",
    "\n",
    "asr_speechbrain_en = SpeechBrainASR(model_name=\"asr-wav2vec2-commonvoice-en\", folder=model_folder / \"speechbrain\")\n",
    "asr_speechbrain_en.transcribe(filepath=data_folder / test_df[test_df.locale == \"en\"].filename.values[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facebook Wav2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import torch\n",
    "import soundfile as sf\n",
    "\n",
    "\n",
    "class FacebookWave2VecASR:\n",
    "    def __init__(self):\n",
    "        super(FacebookWave2VecASR, self).__init__()\n",
    "        self.processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "        self.model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "    def transcribe(self, filepath: Path) -> str:\n",
    "        with filepath.open(\"rb\") as fh:\n",
    "            data, sr = sf.read(fh)\n",
    "            inputs = self.processor([data], sampling_rate=sr, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                y = self.model(**inputs)\n",
    "                logits = y.logits\n",
    "                ids = torch.argmax(logits, dim=-1)\n",
    "                return self.processor.batch_decode(ids)[0]\n",
    "\n",
    "\n",
    "asr_fbwav2vec_en = FacebookWave2VecASR()\n",
    "asr_fbwav2vec_en.transcribe(filepath=data_folder / test_df[test_df.locale == \"en\"].filename.values[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcription\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Speech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse(df: pd.DataFrame, folder: Path, asr_engine: AbstractASR) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for ix, filename in df.filename.iteritems():\n",
    "        y_pred = asr_engine.transcribe(filepath=folder / filename)\n",
    "        df.at[ix, \"y_pred\"] = y_pred\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_deepspeech_en_df = analyse(test_df[test_df.locale == \"en\"], data_folder, asr_deepspeech_en)\n",
    "results_deepspeech_en_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_deepspeech_it_df = analyse(test_df[test_df.locale == \"it\"], data_folder, asr_deepspeech_it)\n",
    "results_deepspeech_it_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_deepspeech_es_df = analyse(test_df[test_df.locale == \"es\"], data_folder, asr_deepspeech_es)\n",
    "results_deepspeech_es_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative (DeepSpeech, SpeechBrain, FBWav2Vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(df: pd.DataFrame, asr_engines: Dict[str, AbstractASR]) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for ix, filename in tqdm(df.filename.iteritems(), total=len(df.filename)):\n",
    "        for asr_name, asr_engine in asr_engines.items():\n",
    "            y_pred = asr_engine.transcribe(filepath=data_folder / filename)\n",
    "            df.at[ix, f\"y_pred_{asr_name}\"] = normalise_text(y_pred)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparative_df = compare(\n",
    "    df=test_df[test_df.locale == \"en\"],\n",
    "    asr_engines={\n",
    "        \"deepspeech\": asr_deepspeech_en,\n",
    "        \"speechbrain\": asr_speechbrain_en,\n",
    "        \"fbwav2vec\": asr_fbwav2vec_en,\n",
    "    },\n",
    ")\n",
    "comparative_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation (WER, or \"Word Error Rate\")\n",
    "\n",
    "Calculated using the following formula:\n",
    "\n",
    "$WER = \\frac{S + D+ I}{N} \\times 100$\n",
    "\n",
    "where:\n",
    "* $S$ is the number of substitutions\n",
    "* $D$ is the number of deletions\n",
    "* $I$ is the number of insertions\n",
    "* $N$ is the number of words in your \"y_true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    df: pd.DataFrame\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df\n",
    "\n",
    "    def evaluate(self, asr_name: str = None) -> pd.DataFrame:\n",
    "        y_pred_column = \"y_pred\"\n",
    "        if asr_name:\n",
    "            y_pred_column = f\"{y_pred_column}_{asr_name}\"\n",
    "        s, d, i, n = Evaluator._calculate_sdi(self.df.y_true, self.df[y_pred_column])\n",
    "        return (s + d + i) / n\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_sdi(y_true_series: pd.Series, y_pred_series: pd.Series) -> Tuple[int, int, int, int]:\n",
    "        s, d, i, n = 0, 0, 0, 0\n",
    "        for y_true, y_pred in zip(y_true_series, y_pred_series):\n",
    "            y_true = Evaluator._tokenise(y_true)\n",
    "            y_pred = Evaluator._tokenise(y_pred)\n",
    "            n += len(y_true)\n",
    "            s += sum(1 for i, j in zip(y_true, y_pred) if i != j)\n",
    "            d += sum(1 for i in y_true if i not in y_pred)\n",
    "            i += sum(1 for i in y_pred if i not in y_true)\n",
    "        return s, d, i, n\n",
    "\n",
    "    @staticmethod\n",
    "    def _tokenise(s: str) -> List[str]:\n",
    "        return s.split(\" \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Speech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluator(results_deepspeech_en_df).evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluator(results_deepspeech_it_df).evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluator(results_deepspeech_es_df).evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative (DeepSearch, SpeechBrain, FBWav2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluator(comparative_df).evaluate(asr_name='deepspeech')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluator(comparative_df).evaluate(asr_name='speechbrain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluator(comparative_df).evaluate(asr_name='fbwav2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2eae4de737fc7bb56fc51d9b7910e57c2c74fec1d168a31b06f7eed62cd7bffe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
